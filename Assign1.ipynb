{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e5db9630-61f9-4eb2-b0af-629df7ddb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8224b75b-27c4-4726-bf81-1a4af45e540a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53bac95f-c265-406c-99b2-27fb844d3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. The length (in words)\n",
    "def getLength(corpus):\n",
    "    return len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "43c03718-9043-41f6-a1f6-3d3ba62e4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. The lexical diversity\n",
    "def getLexicalDiversity(corpus):\n",
    "    #case normalization\n",
    "    return len(set(w.lower() for w in corpus if w.isalpha())) / len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a9882cea-0428-4172-8d8a-6b2c4d768e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Top 10 most frequent words and their counts\n",
    "def getTop10(corpus):\n",
    "    #case normalization\n",
    "    sorted = [w.lower() for w in corpus if w.isalpha()]\n",
    "    \n",
    "    # Frequency distribution\n",
    "    fdist = FreqDist(sorted)\n",
    "    \n",
    "    return fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bd74f165-4acc-4e2b-8d49-f599d24cf644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most frequent words and their counts (with more than 3 characters)\n",
    "def getTop10W3(corpus):\n",
    "    # Case normalization and more than 3 characters\n",
    "    filtered_words = [w.lower() for w in corpus if w.isalpha() and len(w) > 3]\n",
    "    \n",
    "    # Frequency distribution\n",
    "    fdist = FreqDist(filtered_words)\n",
    "\n",
    "    return fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e69bd77c-6e41-40fd-b9d9-e048d6ff6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most frequent words and their counts, excluding stopwords\n",
    "\"\"\"\n",
    "stopWords = {'is', 'that', 'to', 'the', 'and', 'of', 'a', 'in', 'it', 'for', 'on', 'with', 'as', 'by', 'at', 'this', 'an', 'be'}\n",
    "def getTop10(corpus):\n",
    "    # Case normalization and stopwords\n",
    "    filtered_words = [w.lower() for w in corpus if w.isalpha() and w.lower() not in stopWords]\n",
    "    \n",
    "    # Frequency distribution\n",
    "    fdist = FreqDist(filtered_words)\n",
    "    \n",
    "    return fdist.most_common(10)\n",
    "    \"\"\"\n",
    "# just knowing nltk have stopword liberay \n",
    "def getTop10StopWord(corpus):\n",
    "    # Case normalization and stopwords\n",
    "    filtered_words = [w.lower() for w in corpus if w.isalpha() and w.lower() not in stopWords]\n",
    "    \n",
    "    # Frequency distribution\n",
    "    fdist = FreqDist(filtered_words)\n",
    "    \n",
    "    return fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a85ec856-fc6e-4a29-99f9-8711b8925e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Words that are at least 10 characters long and their counts\n",
    "def getLongWords(corpus):\n",
    "    #case normalization that are at least 10 characters\n",
    "    longWords = [w.lower() for w in corpus if len(w) >= 10 and w.isalpha()]\n",
    "    longWordsFdist = FreqDist(longWords)\n",
    "    return longWordsFdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "67daaa23-e6a8-4b69-b9c4-7a4509f1397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. The longest sentence (type the sentence and give the number of words)\n",
    "def getLongestSentence(corpus):\n",
    "    #sentance tokenize the whole corpus\n",
    "    sentences = sent_tokenize(\" \".join(corpus))\n",
    "    #comparing lengths\n",
    "    longestSentence = max(sentences, key=lambda s: len(s.split()))\n",
    "    return len(longestSentence.split()), longestSentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "78a3aaa4-34b8-4ddb-aed4-e674066ac4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef getStemmed(sentence):\\n    ps = PorterStemmer()  \\n    # Tokenize\\n    stemmed_sentence = [ps.stem(w) for w in word_tokenize(sentence.lower())]\\n    # re-string\\n    return \" \".join(stemmed_sentence)\\n   '"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. A stemmed version of the longest sentence\n",
    "# porter stemmer did work for certen words \n",
    "\"\"\"\n",
    "def getStemmed(sentence):\n",
    "    ps = PorterStemmer()  \n",
    "    # Tokenize\n",
    "    stemmed_sentence = [ps.stem(w) for w in word_tokenize(sentence.lower())]\n",
    "    # re-string\n",
    "    return \" \".join(stemmed_sentence)\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1b211b79-ac29-4a35-9a3d-df74e4c937a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef getStemmed(sentence):\\n    ss = SnowballStemmer(\"english\") \\n    # Tokenize normalization\\n    stemmed_sentence = [ss.stem(w) for w in word_tokenize(sentence.lower())]\\n    # re-string\\n    return \" \".join(stemmed_sentence)\\n    '"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. A stemmed version of the longest sentence\n",
    "# snowball stemmer \n",
    "\"\"\"\n",
    "def getStemmed(sentence):\n",
    "    ss = SnowballStemmer(\"english\") \n",
    "    # Tokenize normalization\n",
    "    stemmed_sentence = [ss.stem(w) for w in word_tokenize(sentence.lower())]\n",
    "    # re-string\n",
    "    return \" \".join(stemmed_sentence)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b3a7c60e-31e6-42a3-af42-f7eca06d8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. A stemmed version of the longest sentence\n",
    "# there are still some problem with lancaster stemmer but accuracy is increased\n",
    "\n",
    "def getStemmed(sentence):\n",
    "    st = LancasterStemmer()  \n",
    "    # Tokenize normalization\n",
    "    stemmed_sentence = [st.stem(w) for w in word_tokenize(sentence.lower())]\n",
    "    # re-string\n",
    "    return \" \".join(stemmed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "db360b54-3272-45e6-bb3c-3d31ad3400da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with open('tester3.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "tokens = word_tokenize(content)\n",
    "\n",
    "# Get functions\n",
    "length = getLength(tokens)\n",
    "lexical_diversity = getLexicalDiversity(tokens)\n",
    "top_10 = getTop10(tokens)\n",
    "top_10_w3 = getTop10W3(tokens)\n",
    "top_10_stopwords = getTop10StopWord(tokens)\n",
    "long_words = getLongWords(tokens)\n",
    "longest_sentence_length, longest_sentence = getLongestSentence(tokens)\n",
    "stemmed_sentence = getStemmed(longest_sentence)\n",
    "\n",
    "# Write results to txt\n",
    "with open('output3.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(f\"1. Length (in words): {length}\\n\")\n",
    "    output_file.write(f\"2. Lexical Diversity: {lexical_diversity}\\n\")\n",
    "    output_file.write(\"3. Top 10 Most Frequent Words:\\n\")\n",
    "    for word, count in top_10:\n",
    "        output_file.write(f\"   {word}: {count}\\n\")\n",
    "    \n",
    "    output_file.write(\"5. Top 10 Most Frequent Words Excluding Stopwords:\\n\")\n",
    "    for word, count in top_10_stopwords:\n",
    "        output_file.write(f\"   {word}: {count}\\n\")\n",
    "    \n",
    "    output_file.write(\"6. Words that are at Least 10 Characters Long:\\n\")\n",
    "    for word, count in long_words:\n",
    "        output_file.write(f\"   {word}: {count}\\n\")\n",
    "    \n",
    "    output_file.write(f\"7. Longest Sentence Length: {longest_sentence_length}\\n\")\n",
    "    output_file.write(f\"   Longest Sentence: \\\"{longest_sentence}\\\"\\n\")\n",
    "    output_file.write(f\"8. Stemmed Version of the Longest Sentence:\\n\")\n",
    "    output_file.write(f\"   \\\"{stemmed_sentence}\\\"\\n\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39f2e8-0eb9-4e39-88bc-e3801acee617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62573048-744e-4068-b526-27b0c507f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
